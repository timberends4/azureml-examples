{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AzureML in a Day\n",
                "\n",
                "Learn how a data scientist uses Azure Machine Learning (Azure ML) to train a model, then use the model for prediction. This tutorial will help you become familiar with the core concepts of Azure ML and their most common usage. \n",
                "\n",
                "You'll learn how to submit a *command job* to run your *training script*, configured with the *job environment* necessary to run the script.\n",
                "\n",
                "The training script handles the data preparation, then trains and registers a model. Once you have the model, you'll *deploy* it as an *endpoint*, then call the endpoint for *inferencing*.\n",
                "\n",
                "The steps you'll take are:\n",
                "\n",
                "> * Connect to your Azure ML workspace\n",
                "> * Create your job environment\n",
                "> * Create your training script\n",
                "> * Create and run your command job to run the training script, configured with the appropriate job environment\n",
                "> * View the output of your training script\n",
                "> * Deploy the newly-trained model as an endpoint\n",
                "> * Call the Azure ML endpoint for inferencing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "* An Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
                "* A working Azure ML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
                "* An Azure Machine Learning [workspace]()\n",
                "* A workspace and compute instance which you can create by  completing the [Quickstart: Get started with Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/quickstart-create-resources#create-compute-instance)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Connect to the workspace\n",
                "\n",
                "Before you dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
                "\n",
                "We're using `DefaultAzureCredential` to get access to workspace. \n",
                "`DefaultAzureCredential` is used to handle most Azure SDK authentication scenarios. \n",
                "\n",
                "Reference for more available credentials if it doesn't work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/python/api/azure-identity/azure.identity?view=azure-python)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "name": "credential"
            },
            "outputs": [],
            "source": [
                "# Handle to the workspace\n",
                "from azure.ai.ml import MLClient\n",
                "\n",
                "# Authentication package\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "credential = DefaultAzureCredential()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you want to use a browser to login and authenticate, you can use the following code instead. In this example, you'll use the `DefaultAzureCredential`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle to the workspace\n",
                "from azure.ai.ml import MLClient\n",
                "\n",
                "from azure.identity import InteractiveBrowserCredential\n",
                "credential = InteractiveBrowserCredential()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
                "\n",
                "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
                "1. Copy the value for workspace, resource group and subscription ID into the code.  \n",
                "1. You'll need to copy one value, close the area and paste, then come back for the next one.\n",
                "\n",
                "![image of workspace credentials](media\\find-credentials.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "name": "ml_client"
            },
            "outputs": [],
            "source": [
                "# Get a handle to the workspace\n",
                "ml_client = MLClient(\n",
                "    credential=credential,\n",
                "    subscription_id=\"3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6\",\n",
                "    resource_group_name=\"geris-ml-analysis-dev\",\n",
                "    workspace_name=\"mlw-gerisml-akpt-dev\",\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The result is a handler to the workspace that you'll use to manage other resources and jobs.\n",
                "\n",
                "> [!IMPORTANT]\n",
                "> Creating MLClient will not connect to the workspace. The client initialization is lazy, it will wait for the first time it needs to make a call (in the notebook below, that will happen during job environment creation)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create a job environment\n",
                "\n",
                "To run your AzureML job, you'll need an [environment](https://docs.microsoft.com/azure/machine-learning/concept-environments). An environment lists the software runtime and libraries that you want installed on the compute where youâ€™ll be training. It's similar to your Python environment on your local machine.\n",
                "\n",
                "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
                "\n",
                "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
                "\n",
                "First, create a directory to store the file in."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "name": "dependencies_dir"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "dependencies_dir = \"./dependencies\"\n",
                "os.makedirs(dependencies_dir, exist_ok=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, create the file in the dependencies directory. The cell below uses IPython magic to write the file into the directory you just created."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "name": "write_model"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting ./dependencies/conda.yaml\n"
                    ]
                }
            ],
            "source": [
                "%%writefile {dependencies_dir}/conda.yaml\n",
                "  name: model-env\n",
                "  channels:\n",
                "    - pytorch\n",
                "    - conda-forge\n",
                "  dependencies:\n",
                "    - python=3.10\n",
                "    - numpy=1.26.4\n",
                "    - pip=24.0\n",
                "    - scikit-learn=1.5.1\n",
                "    - scipy=1.10\n",
                "    - pandas=1.5.3\n",
                "    - pytorch=2.2.1\n",
                "    - pip:\n",
                "      - inference-schema[numpy-support]==1.3.0\n",
                "      - xlrd==2.0.1\n",
                "      - mlflow\n",
                "      - azureml-mlflow\n",
                "      - psutil\n",
                "      - tqdm\n",
                "      - ipykernel~=6.29\n",
                "      - matplotlib\n",
                "\n",
                "  "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "The specification contains some usual packages, that you'll use in your job (numpy, pip).\n",
                "\n",
                "Reference this *yaml* file to create and register this custom environment in your workspace:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "name": "custom_env_name"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Environment with name ml-torch-lr is registered to workspace, the environment version is 1\n"
                    ]
                }
            ],
            "source": [
                "from azure.ai.ml.entities import Environment\n",
                "\n",
                "custom_env_name = \"ml-torch-lr\"\n",
                "\n",
                "pipeline_job_env = Environment(\n",
                "    name=custom_env_name,\n",
                "    description=\"Custom environment for ML pipeline job\",\n",
                "    tags={\"project\": \"geris-ml-lr\", \"type\": \"pipeline\"},\n",
                "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
                "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04:latest\",\n",
                ")\n",
                "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
                "\n",
                "print(\n",
                "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What is a command job?\n",
                "\n",
                "You'll create an Azure ML *command job* to train a model for credit default prediction. The command job is used to run a *training script* in a specified environment on serverless compute.  You've already created the environment.  Next you'll create the training script.\n",
                "\n",
                "The *training script* handles the data preparation, training and registering of the trained model. In this tutorial, you'll create a Python training script.\n",
                "\n",
                "Command jobs can be run from CLI, Python SDK, or studio interface. In this tutorial, you'll use the Azure ML Python SDK v2 to create and run the command job.\n",
                "\n",
                "After running the training job, you'll deploy the model, then use it to produce a prediction."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create training script\n",
                "\n",
                "Let's start by creating the training script - the *main.py* Python file.\n",
                "\n",
                "First create a source folder for the script:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "name": "train_src_dir"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "train_src_dir = \"./src\"\n",
                "os.makedirs(train_src_dir, exist_ok=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This script handles the preprocessing of the data, splitting it into test and train data. It then consumes this data to train a tree based model and return the output model. \n",
                "\n",
                "[MLFlow](https://learn.microsoft.com/azure/machine-learning/how-to-log-mlflow-models) will be used to log the parameters and metrics during our pipeline run. \n",
                "\n",
                "The cell below uses IPython magic to write the training script into the directory you just created."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {
                "name": "write_main"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting ./src/main.py\n"
                    ]
                }
            ],
            "source": [
                "%%writefile {train_src_dir}/main.py\n",
                "import os\n",
                "import argparse\n",
                "import pandas as pd\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import tempfile\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.distributions import MultivariateNormal\n",
                "import numpy as np\n",
                "\n",
                "def main():\n",
                "    \"\"\"Main function of the script.\"\"\"\n",
                "\n",
                "    # input and output arguments\n",
                "    parser = argparse.ArgumentParser()\n",
                "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
                "\n",
                "    parser.add_argument(\"--test_end\", type=str, required=False, default='2023-01-01', help=\"date to split train/test data\")\n",
                "    parser.add_argument(\"--epochs\", required=False, default=1000, type=int)\n",
                "    parser.add_argument(\"--n_samples\", required=False, default=100, type=int)\n",
                "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
                "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
                "    args = parser.parse_args()\n",
                "   \n",
                "    # Start Logging\n",
                "    mlflow.start_run()\n",
                "\n",
                "    ###################\n",
                "    #<prepare the data>\n",
                "    ###################\n",
                "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
                "\n",
                "    weekly_data = pd.read_json(args.data, orient=\"records\")\n",
                "\n",
                "    mlflow.log_metric(\"num_weeks\", weekly_data.shape[0])\n",
                "\n",
                "    import tempfile\n",
                "    # Log first few rows as text artifact\n",
                "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as tmpfile:\n",
                "        tmpfile.write(weekly_data.head(5).to_string(index=False))\n",
                "        tmpfile_path = tmpfile.name\n",
                "\n",
                "    mlflow.log_artifact(tmpfile_path, artifact_path=\"data_preview\")\n",
                "\n",
                "    # Clean up temporary file\n",
                "    os.remove(tmpfile_path)\n",
                "  \n",
                "    ####################\n",
                "    #</prepare the data>\n",
                "    ####################\n",
                "\n",
                "    import logging\n",
                "    import sys\n",
                "\n",
                "\n",
                "    # # Configure logging to write to stdout (this ends up in std_log.txt in Azure ML)\n",
                "    # logging.basicConfig(\n",
                "    #     stream=sys.stdout,\n",
                "    #     level=logging.INFO,\n",
                "    #     format='%(asctime)s - %(levelname)s - %(message)s'\n",
                "    # )\n",
                "    # logger = logging.getLogger(__name__)\n",
                "\n",
                "    weekly_data = weekly_data.set_index(pd.to_datetime(weekly_data['date']))\n",
                "    weekly_data_train = weekly_data[weekly_data.index < pd.to_datetime(args.test_end)]\n",
                "    weekly_data_test = weekly_data[weekly_data.index >= pd.to_datetime(args.test_end)]\n",
                "    \n",
                "    mlflow.log_metric(\"num_weeks_train\", weekly_data_train.shape[0])\n",
                "    mlflow.log_metric(\"num_weeks_test\", weekly_data_test.shape[0])\n",
                "    \n",
                "    # logger.info(f\"Columns in the dataset: {weekly_data.columns}\")\n",
                "\n",
                "    cols_fut_true = ['Butter_EEX_4_weeks_ahead', 'Butter_EEX_8_weeks_ahead', 'Butter_EEX_12_weeks_ahead', 'SMP_food_EEX_4_weeks_ahead', 'SMP_food_EEX_8_weeks_ahead', 'SMP_food_EEX_12_weeks_ahead']\n",
                "    cols_fut_estimate = ['Fut_butter_4', 'Fut_butter_8', 'Fut_butter_12', 'Fut_smp_4', 'Fut_smp_8', 'Fut_smp_12']\n",
                "    cols_x = ['Butter_EEX', 'SMP_food_EEX']\n",
                "\n",
                "    cols_y = ['Gouda_EEX']\n",
                "    cols_target = ['Gouda_EEX_4_weeks_ahead', 'Gouda_EEX_8_weeks_ahead', 'Gouda_EEX_12_weeks_ahead']\n",
                "\n",
                "    fut_true_train = torch.tensor(weekly_data_train[cols_fut_true].values, dtype=torch.float32)\n",
                "    fut_true_test = torch.tensor(weekly_data_test[cols_fut_true].values, dtype=torch.float32)\n",
                "\n",
                "    fut_estimate_train = torch.tensor(weekly_data_train[cols_fut_estimate].values, dtype=torch.float32)\n",
                "    fut_estimate_test = torch.tensor(weekly_data_test[cols_fut_estimate].values, dtype=torch.float32)\n",
                "\n",
                "    x_train = torch.tensor(weekly_data_train[cols_x].values, dtype=torch.float32)\n",
                "    x_test = torch.tensor(weekly_data_test[cols_x].values, dtype=torch.float32)\n",
                "    y_train = torch.tensor(weekly_data_train[cols_y].values, dtype=torch.float32)\n",
                "    y_test = torch.tensor(weekly_data_test[cols_y].values, dtype=torch.float32)\n",
                "\n",
                "    fut_target_train = torch.tensor(weekly_data_train[cols_target].values, dtype=torch.float32)\n",
                "    fut_target_test = torch.tensor(weekly_data_test[cols_target].values, dtype=torch.float32)\n",
                "\n",
                "    dates_train  = weekly_data_train.index.values\n",
                "    dates_test  = weekly_data_test.index.values\n",
                "\n",
                "    ###################\n",
                "    #</train the model>\n",
                "    ###################\n",
                "    class LearnableCovarianceFreeForm(nn.Module):\n",
                "        def __init__(self, dim):\n",
                "            super().__init__()\n",
                "            # Parameterize the Cholesky decomposition of the covariance matrix\n",
                "            self.lower_triangular = nn.Parameter(torch.eye(dim))\n",
                "\n",
                "        def forward(self):\n",
                "            lower = torch.tril(self.lower_triangular)\n",
                "            diag = torch.diag(lower)\n",
                "            lower = lower + torch.diag(torch.abs(diag) + 1e-6)\n",
                "            covariance_matrix = torch.matmul(lower, lower.T)\n",
                "            return covariance_matrix\n",
                "\n",
                "        def log_prob(self, mean, actual_values):\n",
                "            covariance = self.forward()\n",
                "            dist = MultivariateNormal(mean, covariance)\n",
                "            return dist.log_prob(actual_values)\n",
                "\n",
                "        def sample(self, num_samples, mean):\n",
                "            covariance = self.forward()\n",
                "            dist = MultivariateNormal(mean, covariance)\n",
                "            return dist.sample((num_samples,))\n",
                "\n",
                "        def get_learned_covariance(self):\n",
                "            return self.forward().detach()\n",
                "\n",
                "\n",
                "    class SimpleLRModel(nn.Module):\n",
                "        def __init__(self, input_dim, future_dim):\n",
                "            super(SimpleLRModel, self).__init__()\n",
                "            self.linear = nn.Linear(input_dim, 1, bias=False)\n",
                "            self.covariance_learner = LearnableCovarianceFreeForm(future_dim)\n",
                "\n",
                "        def forward(self, x):\n",
                "            return self.linear(x)\n",
                "        \n",
                "        def combined_log_prob(self, x_future, x_future_actual, y_pred, y_true):\n",
                "            # Calculate the log probability of the future estimates given the actual values\n",
                "            log_prob_future = self.covariance_learner.log_prob(x_future, x_future_actual)\n",
                "            # Calculate the log probability of the target variable given the future estimates\n",
                "            log_prob_target = self._likelihood(y_pred, y_true)\n",
                "            return log_prob_future + log_prob_target\n",
                "            \n",
                "        def _likelihood(self, y_pred, y_true):\n",
                "            # Assuming a Gaussian likelihood for the target variable\n",
                "            mse = torch.mean((y_pred - y_true) ** 2, dim=1)\n",
                "            return - mse\n",
                "        \n",
                "        def get_learned_future_covariance(self):\n",
                "            return self.covariance_learner.get_learned_covariance()\n",
                "\n",
                "        def sample(self, num_samples, butter_fut, smp_fut, time_steps):\n",
                "            pred_samples = torch.zeros(num_samples, time_steps)\n",
                "            butter_samples = torch.zeros(num_samples, len(butter_fut))\n",
                "            smp_samples = torch.zeros(num_samples, len(smp_fut))\n",
                "\n",
                "            futs_mean = torch.cat((butter_fut, smp_fut), dim=0)\n",
                "            \n",
                "            for i in range(num_samples):\n",
                "                fut_values = self.covariance_learner.sample(1, futs_mean).squeeze(0)\n",
                "                \n",
                "                butter_fut_sampled = fut_values[:butter_fut.shape[0]]  # Use input shape\n",
                "                smp_fut_sampled = fut_values[butter_fut.shape[0]:]      # Use input shape\n",
                "\n",
                "                butter_samples[i] = butter_fut_sampled\n",
                "                smp_samples[i] = smp_fut_sampled\n",
                "\n",
                "                single_pred = torch.zeros(time_steps)\n",
                "                for t in range(time_steps):\n",
                "                    combined_features = torch.cat((butter_fut_sampled[t].unsqueeze(0), smp_fut_sampled[t].unsqueeze(0)), dim=0)\n",
                "\n",
                "                    single_pred[t] = self.linear(combined_features).squeeze(0) # Squeeze the output\n",
                "\n",
                "                pred_samples[i] = single_pred\n",
                "\n",
                "            return pred_samples, butter_samples, smp_samples\n",
                "\n",
                "    num_fut_values = len(cols_fut_true)\n",
                "\n",
                "    # Instantiate the model and optimizer\n",
                "    model = SimpleLRModel(input_dim=len(cols_x), future_dim=num_fut_values)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
                "\n",
                "    # Training loop\n",
                "    num_epochs = args.epochs\n",
                "\n",
                "    for epoch in range(num_epochs):\n",
                "        optimizer.zero_grad()\n",
                "        pred = model(x_train)\n",
                "\n",
                "        loss = -torch.sum(model.combined_log_prob(fut_estimate_train, fut_true_train, pred, fut_target_train))\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        if (epoch + 1) % 50 == 0:\n",
                "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
                "\n",
                "    print(\"MAE of index transformation: \", np.mean(np.abs(pred[:, 0].detach().numpy() - fut_target_train[:, 0].numpy())))\n",
                "\n",
                "    print(\"Testting on test set\")\n",
                "    with torch.no_grad():\n",
                "        model.eval()\n",
                "\n",
                "        pred_test = model(x_test)\n",
                "        test_loss = -torch.sum(model.combined_log_prob(fut_estimate_test, fut_true_test, pred_test, fut_target_test))\n",
                "        print(f'Test Loss: {test_loss.item():.4f}')\n",
                "        print(\"MAE of index transformation: \", np.mean(np.abs(pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())))\n",
                "        print(\"MSE of index transformation: \", np.mean((pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())**2))\n",
                "        print(\"RMSE of index transformation: \", np.sqrt(np.mean((pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())**2)))\n",
                "\n",
                "\n",
                "    mlflow.log_metric(\"MAE train\", np.mean(np.abs(pred[:, 0].detach().numpy() - fut_target_train[:, 0].numpy())))\n",
                "    mlflow.log_metric(\"MAE test\", np.mean(np.abs(pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())))\n",
                "    mlflow.log_metric(\"RMSE test\", np.sqrt(np.mean((pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())**2)))\n",
                "    mlflow.log_metric(\"MSE test\", np.mean((pred_test[:, 0].detach().numpy() - fut_target_test[:, 0].numpy())**2))\n",
                "\n",
                "    ##########################\n",
                "    #<save and register model>\n",
                "    ##########################\n",
                "    # Registering the model to the workspace\n",
                "    print(\"Registering the model via MLFlow\")\n",
                "    mlflow.pytorch.log_model(\n",
                "        model, \n",
                "        registered_model_name=args.registered_model_name,\n",
                "        artifact_path=\"model_lr_generative\",\n",
                "    )\n",
                "\n",
                "    \n",
                "    ###########################\n",
                "    #</save and register model>\n",
                "    ###########################\n",
                "    \n",
                "    # Stop Logging\n",
                "    mlflow.end_run()\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see in this script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\n",
                "\n",
                "## Configure the command\n",
                "\n",
                "Now that you have a script that can perform the desired tasks, you'll use the general purpose **command** that can run command line actions. This command line action can be directly calling system commands or by running a script. \n",
                "\n",
                "Here, you'll create input variables to specify the input data, split ratio, learning rate and registered model name.  The command script will:\n",
                "* Use the environment created earlier - you can use the `@latest` notation to indicate the latest version of the environment when the command is run.\n",
                "* Configure some metadata like display name, experiment name etc. An *experiment* is a container for all the iterations you do on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
                "* Configure the command line action itself - `python main.py` in this case. The inputs/outputs are accessible in the command via the `${{ ... }}` notation.\n",
                "* In this sample, we access the data from a file on the internet. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 111,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 112,
            "metadata": {
                "name": "registered_model_name"
            },
            "outputs": [],
            "source": [
                "from azure.ai.ml import command\n",
                "from azure.ai.ml import Input\n",
                "\n",
                "registered_model_name = \"pytorch_lr_model\"\n",
                "\n",
                "job = command(\n",
                "    inputs=dict(\n",
                "        data=Input(\n",
                "            type=\"uri_file\",\n",
                "            path=\"C:/Users/TBerends/OneDrive - Geris BV/Documents/GitHub/Geris-Dairy-Solutions-BV/Trading/Risk/Flows/weekly_data.json\",\n",
                "        ),\n",
                "        learning_rate=0.1,\n",
                "        registered_model_name=registered_model_name,\n",
                "        test_end=\"2024-01-01\",\n",
                "        epochs=1000,\n",
                "    ),\n",
                "    code=\"./src/\",  # location of source code\n",
                "    command=\"python main.py --data ${{inputs.data}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
                "    environment=\"ml-torch-lr:1\",\n",
                "    experiment_name=\"train_model_pytorch\",\n",
                "    display_name=\"train_model_pytorch\",\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Submit the job \n",
                "\n",
                "It's now time to submit the job to run in AzureML. This time you'll use `create_or_update`  on `ml_client.jobs`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 113,
            "metadata": {
                "name": "create_job"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mUploading src (0.01 MBs): 100%|##########| 10089/10089 [00:00<00:00, 553574.38it/s]\n",
                        "\u001b[39m\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>train_model_pytorch</td><td>bubbly_rose_prl0gj8z16</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/bubbly_rose_prl0gj8z16?wsid=/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourcegroups/geris-ml-analysis-dev/workspaces/mlw-gerisml-akpt-dev&amp;tid=1e6640a6-0e6e-4ba1-8306-594a8dfc2669\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
                        ],
                        "text/plain": [
                            "Command({'parameters': {}, 'init': False, 'name': 'bubbly_rose_prl0gj8z16', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/Azure/azureml-examples.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'fc11f4f6770ff1cdea59b6966ee9e4e38fa2335b', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': None, 'ContentSnapshotId': '2badcc32-19a3-440c-bc01-a0af8c431d94'}, 'print_as_yaml': False, 'id': '/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourceGroups/geris-ml-analysis-dev/providers/Microsoft.MachineLearningServices/workspaces/mlw-gerisml-akpt-dev/jobs/bubbly_rose_prl0gj8z16', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\TBerends\\\\OneDrive - Geris BV\\\\Documents\\\\GitHub\\\\azureml-examples\\\\tutorials\\\\azureml-in-a-day', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000024EDA8890D0>, 'serialize': <msrest.serialization.Serializer object at 0x0000024EDA734DE0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'train_model_pytorch', 'experiment_name': 'train_model_pytorch', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourceGroups/geris-ml-analysis-dev/providers/Microsoft.MachineLearningServices/workspaces/mlw-gerisml-akpt-dev?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/bubbly_rose_prl0gj8z16?wsid=/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourcegroups/geris-ml-analysis-dev/workspaces/mlw-gerisml-akpt-dev&tid=1e6640a6-0e6e-4ba1-8306-594a8dfc2669', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_file', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/cb4a01391b2efb83effd113da903d350/weekly_data.json', 'mode': 'ro_mount'}, 'learning_rate': '0.1', 'registered_model_name': 'pytorch_lr_model', 'test_end': '2024-01-01', 'epochs': '1000'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.bubbly_rose_prl0gj8z16', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000024EDA569BD0>, 'learning_rate': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000024EDA569C80>, 'registered_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000024EDA569D30>, 'test_end': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000024EDA569DE0>, 'epochs': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000024EDA569E90>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x0000024EDA7928D0>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'bubbly_rose_prl0gj8z16', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\TBerends\\\\OneDrive - Geris BV\\\\Documents\\\\GitHub\\\\azureml-examples\\\\tutorials\\\\azureml-in-a-day', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000024EDA8890D0>, 'serialize': <msrest.serialization.Serializer object at 0x0000024EDA7AD860>, 'command': 'python main.py --data ${{inputs.data}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}', 'code': '/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourceGroups/geris-ml-analysis-dev/providers/Microsoft.MachineLearningServices/workspaces/mlw-gerisml-akpt-dev/codes/5de63913-7b7a-4a69-a1ad-01453544560a/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourceGroups/geris-ml-analysis-dev/providers/Microsoft.MachineLearningServices/workspaces/mlw-gerisml-akpt-dev/environments/ml-torch-lr/versions/1', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'train_model_pytorch', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_file', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/cb4a01391b2efb83effd113da903d350/weekly_data.json', 'mode': 'ro_mount'}, 'learning_rate': {'type': 'string', 'default': '0.1'}, 'registered_model_name': {'type': 'string', 'default': 'pytorch_lr_model'}, 'test_end': {'type': 'string', 'default': '2024-01-01'}, 'epochs': {'type': 'string', 'default': '1000'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.bubbly_rose_prl0gj8z16', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourceGroups/geris-ml-analysis-dev/providers/Microsoft.MachineLearningServices/workspaces/mlw-gerisml-akpt-dev?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/bubbly_rose_prl0gj8z16?wsid=/subscriptions/3d6a93ea-a2ba-40b0-bc50-32f9968e6bc6/resourcegroups/geris-ml-analysis-dev/workspaces/mlw-gerisml-akpt-dev&tid=1e6640a6-0e6e-4ba1-8306-594a8dfc2669', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000024EDA8890D0>}, 'instance_id': 'f0d0c87d-edd8-4e84-a31b-67d6cf0f6d5a', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'ml-torch-lr:1', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': {'job_tier': 'null'}, 'parent_job_name': None, 'swept': False})"
                        ]
                    },
                    "execution_count": 113,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ml_client.create_or_update(job)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## View job output and wait for job completion\n",
                "\n",
                "View the job in Azure ML studio by selecting the link in the output of the previous cell. \n",
                "\n",
                "The output of this job will look like this in Azure ML studio. Explore the tabs for various details like metrics, outputs etc. Once completed, the job will register a model in your workspace as a result of training. \n",
                "\n",
                "![Screenshot that shows the job overview](media/view-job.gif \"View the job in studio\")\n",
                "\n",
                "> [!IMPORTANT]\n",
                "> Wait until the status of the job is complete before returning to this notebook to continue. The job will take 2 to 3 minutes to run. It could take longer (up to 10 minutes) if the compute has been scaled down to zero nodes and custom environment is still building.\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Deploy the model as an online endpoint\n",
                "\n",
                "Now deploy your machine learning model as a web service in the Azure cloud, an [`online endpoint`](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
                "\n",
                "To deploy a machine learning service, you usually need:\n",
                "\n",
                "* The model assets (file, metadata) that you want to deploy. You've already registered these assets in your training job.\n",
                "* Some code to run as a service. The code executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model, then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns. With an MLFlow model, as in this tutorial, this script is automatically created for you. Samples of scoring scripts can be found [here](https://github.com/Azure/azureml-examples/tree/sdk-preview/sdk/endpoints/online).\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create a new online endpoint\n",
                "\n",
                "Now that you have a registered model and an inference script, it's time to create your online endpoint. The endpoint name needs to be unique in the entire Azure region. For this tutorial, you'll create a unique name using [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 126,
            "metadata": {
                "name": "online_endpoint_name"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Valid online endpoint name: lrpytorchgenerative63b3dedb\n"
                    ]
                }
            ],
            "source": [
                "import uuid\n",
                "\n",
                "# Generate a valid endpoint name\n",
                "base_name = \"lrpytorchgenerative\"\n",
                "uuid_part = str(uuid.uuid4())[:8]  # Take the first 8 characters of the UUID\n",
                "online_endpoint_name = base_name + \"\" + uuid_part\n",
                "\n",
                "# Step 1: Ensure the name is between 3 and 63 characters\n",
                "if len(online_endpoint_name) > 63:\n",
                "    online_endpoint_name = online_endpoint_name[:63]\n",
                "\n",
                "# Step 2: Ensure it doesn't end with a hyphen\n",
                "online_endpoint_name = online_endpoint_name.rstrip('-')\n",
                "\n",
                "# Step 3: Ensure it doesn't start with a hyphen (just in case)\n",
                "if online_endpoint_name.startswith('-'):\n",
                "    online_endpoint_name = online_endpoint_name[1:]\n",
                "\n",
                "# Step 4: Ensure the name only contains lowercase letters, numbers, and hyphens\n",
                "valid_name = \"\".join(c if c.islower() or c.isdigit() or c == '-' else '' for c in online_endpoint_name)\n",
                "\n",
                "print(f\"Valid online endpoint name: {valid_name}\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> [!NOTE]\n",
                "> Expect the endpoint creation to take approximately 6 to 8 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 127,
            "metadata": {
                "name": "endpoint"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Endpoint lrpytorchgenerative63b3dedb provisioning state: Succeeded\n"
                    ]
                }
            ],
            "source": [
                "from azure.ai.ml.entities import (\n",
                "    ManagedOnlineEndpoint,\n",
                "    ManagedOnlineDeployment,\n",
                "    Model,\n",
                "    Environment,\n",
                ")\n",
                "\n",
                "# create an online endpoint\n",
                "endpoint = ManagedOnlineEndpoint(\n",
                "    name=valid_name,\n",
                "    description=\"this is an online endpoint\",\n",
                "    auth_mode=\"key\",\n",
                "    tags={\n",
                "        \"training_dataset\": \"weekly_data\",\n",
                "        \"model_type\": \"pytorch\",\n",
                "    },\n",
                ")\n",
                "\n",
                "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
                "\n",
                "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you've created an endpoint, you can retrieve it as below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 128,
            "metadata": {
                "name": "retrieve_endpoint"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Endpoint \"lrpytorchgenerative63b3dedb\" with provisioning state \"Succeeded\" is retrieved\n"
                    ]
                }
            ],
            "source": [
                "endpoint = ml_client.online_endpoints.get(name=valid_name)\n",
                "\n",
                "print(\n",
                "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Deploy the model to the endpoint\n",
                "\n",
                "Once the endpoint is created, deploy the model with the entry script. Each endpoint can have multiple deployments. Direct traffic to these deployments can be specified using rules. Here you'll create a single deployment that handles 100% of the incoming traffic. We have chosen a color name for the deployment, for example, *blue*, *green*, *red* deployments, which is arbitrary.\n",
                "\n",
                "You can check the **Models** page on Azure ML studio, to identify the latest version of your registered model. Alternatively, the code below will retrieve the latest version number for you to use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 133,
            "metadata": {
                "name": "latest_model_version"
            },
            "outputs": [],
            "source": [
                "# Let's pick the latest version of the model\n",
                "latest_model_version = max(\n",
                "    [int(m.version) for m in ml_client.models.list(name=  registered_model_name) if m.name == registered_model_name]\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Deploy the latest version of the model.  \n",
                "\n",
                "> [!NOTE]\n",
                "> Expect this deployment to take approximately 6 to 8 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "name": "blue_deployment"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Check: endpoint lrpytorchgenerative63b3dedb exists\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "......................"
                    ]
                }
            ],
            "source": [
                "# picking the model to deploy. Here we use the latest version of our registered model\n",
                "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
                "\n",
                "\n",
                "# create an online deployment.\n",
                "blue_deployment = ManagedOnlineDeployment(\n",
                "    name=\"blue\",\n",
                "    endpoint_name=online_endpoint_name,\n",
                "    model=model,\n",
                "    instance_type=\"Standard_DS3_v2\",\n",
                "    instance_count=1,\n",
                ")\n",
                "\n",
                "blue_deployment = ml_client.begin_create_or_update(blue_deployment).result()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test with a sample query\n",
                "\n",
                "Now that the model is deployed to the endpoint, you can run inference with it.\n",
                "\n",
                "Create a sample request file following the design expected in the run method in the score script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "name": "deploy_dir"
            },
            "outputs": [],
            "source": [
                "deploy_dir = \"./deploy\"\n",
                "os.makedirs(deploy_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "name": "write_sample"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Writing ./deploy/sample-request.json\n"
                    ]
                }
            ],
            "source": [
                "%%writefile {deploy_dir}/sample-request.json\n",
                "{\n",
                "  \"input_data\": {\n",
                "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
                "    \"index\": [0, 1],\n",
                "    \"data\": [\n",
                "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
                "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
                "        ]\n",
                "  }\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "name": "test"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'[1, 0]'"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# test the blue deployment with some sample data\n",
                "ml_client.online_endpoints.invoke(\n",
                "    endpoint_name=online_endpoint_name,\n",
                "    request_file=\"./deploy/sample-request.json\",\n",
                "    deployment_name=\"blue\",\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clean up resources\n",
                "\n",
                "If you're not going to use the endpoint, delete it to stop using the resource.  Make sure no other deployments are using an endpoint before you delete it.\n",
                "\n",
                "\n",
                "> [!NOTE]\n",
                "> Expect this step to take approximately 6 to 8 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "name": "delete_endpoint"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<azure.core.polling._poller.LROPoller at 0x19a7a93f9d0>"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "..."
                    ]
                }
            ],
            "source": [
                "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Learn about creating a multi step pipeline for this script [Create production ML pipelines in a Jupyter notebook](https://github.com/Azure/azureml-examples/blob/main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb)."
            ]
        }
    ],
    "metadata": {
        "description": {
            "description": "Learn how a data scientist uses Azure Machine Learning (Azure ML) to train a model, then use the model for prediction. This tutorial will help you become familiar with the core concepts of Azure ML and their most common usage."
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
